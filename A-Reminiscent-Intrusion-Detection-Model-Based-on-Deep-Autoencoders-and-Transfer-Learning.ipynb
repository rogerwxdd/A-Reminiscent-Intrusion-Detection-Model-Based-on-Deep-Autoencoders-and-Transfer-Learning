{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527b19a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from numpy import average\n",
    "from numpy import array\n",
    "from keras.models import clone_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn\n",
    "\n",
    "EPOCHS = 5\n",
    "NWORKERS = 5\n",
    "EPOCHS_WORKER = 50\n",
    "\n",
    "def fedAvg(members):\n",
    "  n_layers = len(members[0].get_weights())\n",
    "  weights = [1.0/len(members) for i in range(1, len(members)+1)]\n",
    "  avg_model_weights = list()\n",
    "  for layer in range(n_layers):\n",
    "    layer_weights = array([model.get_weights()[layer] for model in members])\n",
    "    avg_layer_weights = average(layer_weights, axis=0, weights=weights)\n",
    "    avg_model_weights.append(avg_layer_weights)\n",
    "  model = clone_model(members[0])\n",
    "  model.set_weights(avg_model_weights)\n",
    "  model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def geraNModelosCopiaFederado(modeloFederado, n):\n",
    "  modelos = [clone_model(modelFederado) for i in range(n)]\n",
    "  for modelo in modelos:\n",
    "    modelo.set_weights(modelFederado.get_weights())\n",
    "    modelo.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "  return modelos\n",
    "\n",
    "caminho_dataset = 'dataset/'\n",
    "dataset = ['data01.csv','data02.csv']\n",
    "\n",
    "for data in dataset:\n",
    "  df = pd.read_csv(caminho_dataset + data)\n",
    "  df = df.sample(frac=1).reset_index(drop=True)\n",
    "  x = df.drop('class', axis=1)\n",
    "  inputs = np.asarray(x)\n",
    "  inputs = MinMaxScaler().fit_transform(inputs)\n",
    "  labels = np.asarray(df['class'])\n",
    "  labels = to_categorical(labels)\n",
    "\n",
    "  lista_dataset_x = []\n",
    "  lista_dataset_y = []\n",
    "  lista_workers = []\n",
    "  lista_dataset_federado = []\n",
    "\n",
    "  for i in range(NWORKERS):\n",
    "    inicio = i * int(len(inputs) / NWORKERS)\n",
    "    fim = (i+1) * int(len(inputs) / NWORKERS)\n",
    "\n",
    "    x = inputs[inicio:fim]\n",
    "    y = labels[inicio:fim]\n",
    "    lista_dataset_x.append(x)\n",
    "    lista_dataset_y.append(y)\n",
    "\n",
    "  modelFederado = Sequential()\n",
    "  modelFederado.add(Dense(500, activation='relu', input_dim=21))\n",
    "  modelFederado.add(Dense(100, activation='relu'))\n",
    "  modelFederado.add(Dense(50, activation='relu'))\n",
    "  modelFederado.add(Dense(2, activation='softmax'))\n",
    "\n",
    "  modelos = geraNModelosCopiaFederado(modelFederado, NWORKERS)\n",
    "  for i in range(1, EPOCHS + 1):\n",
    "    print('Epoca [{:2d}/{:2d}]'.format(i, EPOCHS))\n",
    "    for j in range(len(modelos)):\n",
    "      history = modelos[j].fit(lista_dataset_x[j], lista_dataset_y[j], epochs=EPOCHS_WORKER, verbose=0)\n",
    "      print('\\tTreinou modelo: [{:2d}/{:2d}] da epoca [{:2d}] (acc {:.6f} \\tLoss: {:.6f})'.format(j, len(modelos), i, history.history['accuracy'][0], history.history['loss'][0]))\n",
    "    modelFederado = fedAvg(modelos)\n",
    "    modelos = geraNModelosCopiaFederado(modelFederado, NWORKERS)\n",
    "    \n",
    "  modelFederado.save('saveModel/model' + str(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d3307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "from IPython import embed\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import precision_score, accuracy_score\n",
    "import joblib\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "saida = 'saida_predictions/'\n",
    "caminho = 'predictions/'\n",
    "\n",
    "dataset = ['data01.csv','data02.csv']\n",
    "\n",
    "for data in dataset:\n",
    "    arquivoPredictions = pd.read_csv(caminho + data)\n",
    "    lenArqPred = len(arquivoPredictions)\n",
    "    predNorm = []\n",
    "    predAtk = []\n",
    "    predClasse = []\n",
    "    for predNovo in range(lenArqPred):\n",
    "        predNorm.append(float(arquivoPredictions['prob_norm'][predNovo]))  # NORMAL\n",
    "        predAtk.append(float(arquivoPredictions['prob_atk'][predNovo]))  # ATAQUE\n",
    "        predClasse.append(int(arquivoPredictions['classe'][predNovo]))  # CLASSE\n",
    "\n",
    "    arq_write = open(saida + 'saida_' + data + '.csv', 'w')\n",
    "    arq_write.write('limiarNorm;limiarAtk;TN;FP;FN;TP;REJ;TNR;TPR;AVG;REJ_N;ERRO;\\n')\n",
    "\n",
    "    limiarNorm = 0.0\n",
    "    for limiarNormQtd in range(100): # ZERA LIMIAR PARA TESTAR\n",
    "        limiarAtk = 0.0\n",
    "        for limiarAtkQtd in range(100):\n",
    "\n",
    "            TN = 0\n",
    "            FP = 0\n",
    "            TP = 0\n",
    "            FN = 0\n",
    "            REJ = 0\n",
    "\n",
    "            for pred in range(lenArqPred):\n",
    "                ### PROB 1 == ATK\n",
    "                ### PROB 0 == NORM\n",
    "\n",
    "                if (predAtk[pred] >= predNorm[pred]):\n",
    "                    if (predAtk[pred] >= limiarAtk):\n",
    "                        if (predClasse[pred] == 1):\n",
    "                            TP += 1\n",
    "                        else:\n",
    "                            FP += 1\n",
    "                    else:\n",
    "                        REJ += 1\n",
    "                else:\n",
    "                    if (predNorm[pred] >= limiarNorm):\n",
    "                        if (predClasse[pred] == 0):\n",
    "                            TN += 1\n",
    "                        else:\n",
    "                            FN += 1\n",
    "                    else:\n",
    "                        REJ += 1\n",
    "\n",
    "            try:\n",
    "                TNR = TN / (TN + FP)\n",
    "            except:\n",
    "                TNR = 0\n",
    "            try:\n",
    "                TPR = TP / (TP + FN)\n",
    "            except:\n",
    "                TPR = 0\n",
    "            try:\n",
    "                AVG = (TNR + TPR) / 2\n",
    "            except:\n",
    "                AVG = 0\n",
    "            try:\n",
    "                REJ_N = REJ / (TN + FP + FN + TP + REJ)\n",
    "            except:\n",
    "                REJ_N = 0\n",
    "            try:\n",
    "                ERRO = ((FP / (FP + TN)) + (FN / (FN + TP))) / 2\n",
    "            except:\n",
    "                ERRO = 0\n",
    "\n",
    "            print('Lim_N:' + str(limiarNorm) + ' Lim_A:' + str(limiarAtk) + ' TN:' + str(TN) + ' FP:' + str(\n",
    "                FP) + ' FN:' + str(FN) + ' TP:' + str(TP) + ' REJ:' + str(REJ))\n",
    "            print('ERRO: ' + str(ERRO) + ' | TNR ' + str(TNR) + ' | TPR ' + str(TPR))\n",
    "\n",
    "            embed()\n",
    "\n",
    "            arq_write.write(\n",
    "                str(limiarNorm) + ';' + str(limiarAtk) + ';' +\n",
    "                str(TN) + ';' + str(FP) + ';' + str(FN) + ';' + str(TP) + ';' + str(REJ) + ';' +\n",
    "                str(TNR) + ';' + str(TPR) + ';' + str(AVG) + ';' + str(REJ_N) + ';' + str(ERRO) + ';\\n')\n",
    "\n",
    "            limiarAtk += 0.01\n",
    "        limiarNorm += 0.01\n",
    "    arq_write.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b48b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import embed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from numpy import average\n",
    "from numpy import array\n",
    "from keras.models import clone_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "EPOCHS = 5\n",
    "NWORKERS = 5\n",
    "EPOCHS_WORKER = 50\n",
    "\n",
    "def geraNModelosCopiaFederado(modeloFederado, n):\n",
    "  modelos = [clone_model(modelFederado) for i in range(n)]\n",
    "  for modelo in modelos:\n",
    "    modelo.set_weights(modelFederado.get_weights())\n",
    "    modelo.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "  return modelos\n",
    "\n",
    "def fedAvg(members):\n",
    "  n_layers = len(members[0].get_weights())\n",
    "  weights = [1.0/len(members) for i in range(1, len(members)+1)]\n",
    "  avg_model_weights = list()\n",
    "  for layer in range(n_layers):\n",
    "    layer_weights = array([model.get_weights()[layer] for model in members])\n",
    "    avg_layer_weights = average(layer_weights, axis=0, weights=weights)\n",
    "    avg_model_weights.append(avg_layer_weights)\n",
    "  model = clone_model(members[0])\n",
    "  model.set_weights(avg_model_weights)\n",
    "  model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "\n",
    "caminho_dataset = 'dataset/'\n",
    "dataset = ['data01.csv']\n",
    "modelAntigo = '10'\n",
    "modelNovo = '11'\n",
    "\n",
    "for data in dataset:\n",
    "  df = pd.read_csv(caminho_dataset + data)\n",
    "  df = df.sample(frac=1).reset_index(drop=True)\n",
    "  x = df.drop('class', axis=1)\n",
    "  x = x.drop('MAWILAB_taxonomy', axis=1)\n",
    "  x = x.drop('MAWILAB_label', axis=1)\n",
    "  x = x.drop('MAWILAB_nbDetectors', axis=1)\n",
    "  x = x.drop('MAWILAB_distance', axis=1)\n",
    "  x = x.drop('ORUNADA_numberOfDifferentDestinations', axis=1)\n",
    "  x = x.drop('ORUNADA_numberOfDifferentServices', axis=1)\n",
    "  inputs = np.asarray(x)\n",
    "\n",
    "  inputs = MinMaxScaler().fit_transform(inputs)\n",
    "  labels = np.asarray(df['class'])\n",
    "  labels = to_categorical(labels)\n",
    "\n",
    "  lista_dataset_x = []\n",
    "  lista_dataset_y = []\n",
    "  lista_workers = []\n",
    "  lista_dataset_federado = []\n",
    "\n",
    "  for i in range(NWORKERS):\n",
    "    inicio = i * int(len(inputs) / NWORKERS)\n",
    "    fim = (i+1) * int(len(inputs) / NWORKERS)\n",
    "\n",
    "    x = inputs[inicio:fim]\n",
    "    y = labels[inicio:fim]\n",
    "    lista_dataset_x.append(x)\n",
    "    lista_dataset_y.append(y)\n",
    "\n",
    "  modelFederado = keras.models.load_model('saveModel/model' + modelAntigo)\n",
    "  modelos = geraNModelosCopiaFederado(modelFederado, NWORKERS)\n",
    "\n",
    "  for i in range(1, EPOCHS + 1):\n",
    "    print('Epoca [{:2d}/{:2d}]'.format(i, EPOCHS))\n",
    "    for j in range(1):\n",
    "      history = modelos[j].fit(lista_dataset_x[j], lista_dataset_y[j], epochs=EPOCHS_WORKER, verbose=0)\n",
    "      print('\\tTreinou modelo: [{:2d}/{:2d}] da epoca [{:2d}] (acc {:.6f} \\tLoss: {:.6f})'.format(j, len(modelos), i, history.history['accuracy'][0], history.history['loss'][0]))\n",
    "    modelFederado = fedAvg(modelos)\n",
    "    modelos = geraNModelosCopiaFederado(modelFederado, NWORKERS)\n",
    "\n",
    "  ### SAVE MODEL ###\n",
    "  modelFederado.save('saveModel/model' + modelNovo)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
